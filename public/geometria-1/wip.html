<!DOCTYPE html>
<html lang="it">
	<head>
		<meta charset="UTF-8">
		<meta
		name="viewport"
		content="width=device-width,initial-scale=1.0">
		<meta http-equiv="X-UA-Compatible" content="ie=edge">
		<title>Wip</title>
		<link rel="icon" type="image/png" href="/assets/icon@small.png" />
		<link rel="stylesheet" href="/assets/katex.min.css">
		<link rel="stylesheet" href="/assets/tomorrow.css">
		
		<link rel="stylesheet" href="/assets/fonts/cmun-serif.css">
		
		<link rel="stylesheet" href="/assets/style.css">
	</head>
	<body>
		<center>
    <a 
        href="/geometria-1/index.html"
        title="Geometria 1">Indietro</a>
</center>

<h1 id="endomorfismi">Endomorfismi</h1>
<p>Sia $V$ uno spazio vettoriale su $\KK$, allora l&#39;insieme degli endomorfismi è definito nel seguente modo.</p>
<p>$$
\text{End}(V) = \text{Hom}(V, V)
$$</p>
<p>Questa struttura è in modo naturale una $\KK$-algebra. Ovvero $\text{End}(V)$ è un $\KK$ spazio vettoriale con l&#39;operazione della composizione $\compose$ e valgono le seguenti proprietà</p>
<p>$$
\forall x,y,z \in \text{End}(V)
\quad
\forall \lambda \in \KK
$$</p>
<ul>
<li>$(x + y) \compose z = x \compose z + y \compose z$</li>
<li>$x \compose (y + z) = x \compose y + x \compose z$</li>
<li>$(\lambda x) \compose y = \lambda (x \compose y)$</li>
<li>$x \compose (\lambda y) = \lambda (x \compose y)$</li>
</ul>
<p>$\text{End}(V) \supseteq \text{GL}(V)$ è il <strong>gruppo lineare degli endomorfismi invertibili</strong>.</p>
<h3 id="il-caso-di-nodisp-buffer-11-">Il caso di $\KK^n$</h3>
<p>$$ \text{End}(V) = M(n, \KK) $$</p>
<p>Dove $M(n, \KK)$ è l&#39;insieme delle matrici quadrate. In questo caso la composizione corrisponde al prodotto tra matrici</p>
<p>$$ A \compose B = A B $$</p>
<p>Ed inoltre</p>
<p>$$ \text{GL}(\KK^n) = \text{GL}(n, \KK) $$</p>
<h2 id="relazione-di-equivalenza-sugli-endomorfismi">Relazione di Equivalenza sugli Endomorfismi</h2>
<p>Definiamo ora una specializzazione della relazione della SD-equivalenza.</p>
<p><strong>Def.</strong> $f, g \in \text{End}(V)$ diremo che $g$ è <em>coniugata</em> ad $f$ (e scriveremo $g \sim f$) se</p>
<p>$$
g \sim f \iff \exists h \in \text{GL}(V): \; g = h \compose f \compose h^{-1}
$$</p>
<p>Questa è una relazione di equivalenza e nel corso del capitolo studieremo il quoziente $\text{End}(V)/_{ \sim }$.</p>
<h3 id="caso-delle-matrici">Caso delle Matrici</h3>
<p>Nel caso in cui $V = \KK^n$</p>
<p>$$
B \sim A \iff \exists P \in \text{GL}(n, \KK): \; B = P A P^{-1}
$$</p>
<p>Ciò è equivalente a dire che il seguente diagramma commuta
<img src="/assets/geometria-1/endomorfismi-001.jpg" alt="Latex in arrivo"> </p>
<hr>
<p>In generale possiamo ricondurre lo studio nel caso di $V$ generico a quello in $\KK^n$ nel seguente modo mostrato nel diagramma</p>
<p><img src="/assets/geometria-1/endomorfismi-002.jpg" alt="Latex in arrivo"></p>
<p><strong>Prop.</strong> Quindi possiamo dire che dato $V$ con $\dim V = n$ e dati $f,g \in \text{End}(V)$ i seguenti fatti sono equivalenti tra loro</p>
<ol>
<li><p>$g \sim f$</p>
</li>
<li><p>$\exists \mathscr{B}$ base di $V$ tale che $B = \mathscr{M}_\mathscr{B}^\mathscr{B}(g)$ è simile ad $A = \mathscr{M}_\mathscr{B}^\mathscr{B}(f)$</p>
</li>
<li><p>$\forall \mathscr{B}$ basi di $V$ si ha che $B = \mathscr{M}_\mathscr{B}^\mathscr{B}(g)$ è simile ad $A = \mathscr{M}_\mathscr{B}^\mathscr{B}(f)$</p>
</li>
<li><p>$\exists \mathscr{B}, \mathscr{D}$ basi di $V$ tale che $\mathscr{M}_\mathscr{B}^\mathscr{B}(g) = \mathscr{M}_{\mathscr{D}}^{\mathscr{D}}(f)$</p>
</li>
</ol>
<p>Ci poniamo quindi l&#39;obbiettivo di trovare altri <em>invarianti</em> oltre a $\dim \Image$ e $\text{rango}$ per le relazioni</p>
<p>$$
\text{End}(V)/_{ \sim }
\qquad
\text{M}(n, \KK)/_{ \sim }
$$</p>
<h3 id="esempio-banale">Esempio Banale</h3>
<p>Se $V = \KK^1$ abbiamo che l&#39;insieme degli endomorfismi sarà $M(1, \KK) \simeq \KK$ e quindi la relazione di congruenza diventerà</p>
<p>$$
b \sim a \quad b = h a h^{-1} = h h^{-1} a = a
$$</p>
<p>Quindi abbiamo infinite classi di equivalenza. Se invece analizziamo il problema attraverso il rango di $\begin{bmatrix} a \end{bmatrix}$ questo ci da solo due possibili classi di equivalenza, con $a = 0$ e con $a \neq 0$.</p>
<hr>
<h2 id="autovalori">Autovalori</h2>
<p>Sia $f \in \text{End}(V)$ </p>
<h4 id="autovalore">Autovalore</h4>
<p>$\lambda \in \KK$ è un <strong>autovalore</strong> di $f$ se</p>
<p>$$ \dim \text{Ker}(f - \lambda \, \text{id}_V) > 0 $$</p>
<p>equivalentemente </p>
<p>$$\lambda \text{ è un autovalore } \iff \exists v \neq 0 : f(v) = \lambda v $$</p>
<h4 id="spettro">Spettro</h4>
<p>$$
\text{Spettro}(f) = \{ \lambda \in \KK \taleche \lambda \text{ è un autovalore di } f \}
$$</p>
<h4 id="autospazio">Autospazio</h4>
<p>Un <strong>autospazio</strong> di $f$ relativo ad un autovalore $\lambda \in \text{Spettro}(f)$</p>
<p>$$
V_\lambda \coloneqq V_\lambda(f) \coloneqq \text{Ker} (f - \lambda \, \text{id}_V)
$$</p>
<p>Inoltre i $v \in V_\lambda$ con $v \neq 0$ sono detti <strong>autovettori</strong> di $f$ relativi all&#39;autovalore $\lambda$.</p>
<h4 id="molteplicit-geometrica">Molteplicità Geometrica</h4>
<p>$$
0 < d_\lambda \coloneqq \dim V_\lambda(f)
$$</p>
<p>E&#39; detta la <strong>molteplicità geometrica</strong> dell&#39;autovalore $\lambda$ di $f$.</p>
<h3 id="considerazioni">Considerazioni</h3>
<p><strong>Oss.</strong> Lo spettro può essere vuoto, basti pensare ad esempio alle rotazioni in $\mathbb{R}^2$</p>
<p><strong>Prop.</strong> Siano $f, g \in \text{End}(V)$. Se $g \sim f \implies$</p>
<ol>
<li>$\text{Spettro}(g) = \text{Spettro}(f)$</li>
<li>Se $\lambda \in \text{Spettro}(f) \neq \emptyset \implies d_\lambda(f) = d_\lambda(g)$</li>
</ol>
<p><strong>Dim.</strong> </p>
<ol>
<li><p>Poiché $g \sim f$ abbiamo che $\exists h \in \text{GL}(V) : g = h \compose f \compose h^{-1} \iff g \compose h = h \compose f$.
 Sia $\lambda \in \text{Spettro}(f) : \exists v \neq 0 \quad f(v) = \lambda v$.</p>
<p> $$ (g \compose h)(v) = (h \compose f)(v) = h(\lambda v) = \lambda h (v) $$</p>
<p> $$ (g \compose h)(v) = g(\underbrace{h(v)}_{w}) = \lambda \underbrace{h (v)}_{w} $$</p>
<p> $w \neq 0$ poiché $h \in \text{GL}(V)$ e $v \neq 0$</p>
<p> $$ \implies \text{Spettro}(f) \subseteq \text{Spettro}(g) $$</p>
<p> Ovvero otteniamo che ogni autovalore di $f$ è anche un autovalore di $g$. Poi semplicemente scambiando i ruoli di $f$ e $g$ otteniamo l&#39;ugualianza</p>
<p> $$ \implies \text{Spettro}(f) = \text{Spettro}(g) $$</p>
<p> $\qed$</p>
</li>
<li><p>$\lambda \in \text{Spettro}(f)=\text{Spettro}(g)$. Poiché $g \sim f$ abbiamo che $\exists h \in \text{GL}(V) : g = h \compose f \compose h^{-1}$</p>
<p> $$
	h(V_\lambda(f)) = V_\lambda(g) 
	\qquad
	h(V_\lambda(g)) = V_\lambda(f)
	$$</p>
<p> E poiché $h$ è un isomorfismo abbiamo che le dimensioni sono preservate.</p>
<p> $$
	\implies \dim V_\lambda(f) = \dim V_\lambda(g)
	$$</p>
<blockquote>
<p>TODO: Non mi ispira molto questa dimostrazione di Benedetti, in futuro aggiungerò dettagli.</p>
</blockquote>
</li>
</ol>
<h3 id="passando-alle-matrici-">Passando alle Matrici...</h3>
<p>Se $\lambda \in \text{Spettro}(f)$ e $\dim V = n$ possiamo considerare $A = \mathscr{M}_\mathscr{B}^\mathscr{B}(f)$</p>
<p>$$
\begin{array}{rcl}
V_\mathscr{B} & \xrightarrow{f} & V_\mathscr{B} \\
& &\\
\text{[ ]}_\mathscr{B} \Big\downarrow \; & & \; \Big\downarrow \; \text{[ ]}_\mathscr{B} \\
& & \\
\KK^n & \xrightarrow[A]{} & \KK^n
\end{array}
$$ </p>
<p>intanto possiamo notare subito che $d_\lambda(f) = d_\lambda(A)$ poiché $\mathscr{B}$ è un isomorfismo e preserva la dimensione dell&#39;autospazio $V_\lambda(f)$ quando passa in $\KK^n$. Vogliamo ora mostrare che $\text{Spettro}(f) = \text{Spettro}(A)$</p>
<p><strong>Oss.</strong></p>
<p>$$
v \neq 0: f(v) = \lambda v 
\quad \rightsquigarrow \quad
[v]_\mathscr{B} \neq 0: A [v]_\mathscr{B} = \lambda [v]_\mathscr{B}
$$</p>
<p>$$
V_\lambda(A) = \text{Ker}(A - \lambda I_n)
$$</p>
<p>$$
\implies d_\lambda(A) = n - \text{rango}(A - \lambda I_n)
$$</p>
<h3 id="gli-autovalori-generano-spazi-in-somma-diretta-">Gli autovalori generano spazi in somma diretta (?)</h3>
<p>Consideriamo ora $\text{Spettro}(f) \neq \emptyset$ e siano $\lambda_1, \dots, \lambda_m \in \text{Spettro}(f)$ tali che $\lambda_i \neq \lambda_j$ se $i \neq j$ e siano $v_1, \dots, v_m$ dei rispettivi <em>autovettori</em>.</p>
<p>$$
\forall j = 1 \dots m \quad
v_j \neq 0 \quad
f(v_j) = \lambda_j v_j
$$</p>
<p>Dimostriamo ora che questi autovettori sono linearmente indipendenti. Consideriamo i primi $k$ autovalori (con $k < m$) ed i rispettivi autovettori e mostriamo che anche il $k+1$-esimo è linearmente indipendente.</p>
<p><strong>Dim.</strong></p>
<ol>
<li>Per far partire l&#39;induzione vediamo inizialmente che se $k = 1$, abbiamo che $v_1 \neq 0$ quindi va bene.</li>
<li>Ora se prendiamo $\lambda_1, \dots, \lambda_{k+1}$ ed dei rispettivi autovettori $v_1, \dots, v_{k+1}$
 $$
	\begin{cases}
		a_1 v_1 + \dots + a_{k+1} v_{k+1} = 0 \\
		a_1 \lambda_1 v_1 + \dots + a_{k+1} \lambda_{k+1} v_{k+1} = 0
	\end{cases}
	$$
 Ora moltiplichiamo la prima equazione per $\lambda_{k+1}$ e sottraiamola dalla seconda ed otteniamo così
 $$
	a_1 (\lambda_1 - \lambda_{k+1}) v_1 + \dots + a_{k+1} (\underbrace{\lambda_{k+1} - \lambda_{k+1}}_{=0}) v_{k+1} = 0
	$$
 $$
	\implies a_1 (\lambda_1 - \lambda_{k+1}) v_1 + \dots + a_{k} (\lambda_{k} - \lambda_{k+1}) v_{k} = 0
	$$
 Ora l&#39;ipotesi induttiva ci mostra immediatamente ciò che volevamo dimostrare infatti
 $$
	\begin{cases}
		a_1 (\lambda_1 - \lambda_{k+1}) = 0 \\
		\quad \vdots \\
		a_k (\lambda_k - \lambda_{k+1}) = 0
	\end{cases}
	\iff
	a_1 = \dots = a_k = 0
	$$
 $\qed$</li>
</ol>
<p><strong>Corollario.</strong> $\left|\text{Spettro}(f)\right| \leq \dim V$</p>
<p>Inoltre se $\left|\text{Spettro}(f)\right| = n$, abbiamo $\lambda_1, \dots, \lambda_n$ con $\lambda_i \neq \lambda_j$ se $i \neq j$ e possiamo considerare una base di $V$ fatta di autovettori di $f$.</p>
<p>$$\mathscr{B} = \{ v_1, \dots, v_n \}$$</p>
<p>Ed avremo che</p>
<p>$$
\mathscr{M}_\mathscr{B}^\mathscr{B}(f) =
\begin{bmatrix}
	\lambda_1 &        &         0 \\
			  & \ddots &           \\
	        0 &        & \lambda_n \\
\end{bmatrix}
$$</p>
<p><strong>Lemma./Corollario.</strong></p>
<p>Sia $\text{Spettro}(f) = \{ \lambda_1, \dots, \lambda_m \}$ con $1 < m \leq n = \dim V$ e $\forall j = 1, \dots, m : \lambda_i \neq \lambda_j$ se $i \neq j$ e definiamo 
$$
\underset{(d_1)}{V_{\lambda_1}} + \dots + \underset{(d_m)}{V_{\lambda_m}} \coloneqq \text{Span}(\bigcup_{j=1}^m V_{\lambda_j}(f))
$$
$$
\implies
\text{dim}(V_{\lambda_1} + \dots + V_{\lambda_m}) = \sum_{j=1}^m d_{\lambda_j}
$$</p>
<p>Se abbiamo un endomorfismo $f$ possiamo considerare una base di $V$
$$
\mathscr{A} = \{ \mathscr{B}_1, \dots, \mathscr{B}_m, \mathscr{P} \}
$$
$$
\implies V = V_{\lambda_1} \oplus \dots \oplus V_{\lambda_m} \oplus W
$$</p>
<p>$$
\mathscr{M}_\mathscr{A}^\mathscr{A}(f) =
\begin{bmatrix}
	\lambda_1 I_{d_1} &                   &       0 & ? \\
			          & \lambda_2 I_{d_2} &         & ? \\
	                0 &                   & \ddots  & ? \\
\end{bmatrix}
$$</p>
<p><strong>Def.</strong></p>
<p>$f$ si dice <strong>diagonalizzabile</strong>:</p>
<p>$$
\exists \mathscr{B} \text{ base di } V \text{ tale che } \mathscr{M}_\mathscr{B}^\mathscr{B}(f) \text{ sia diagonale } 
$$
$$ \Updownarrow $$
$$ 
\exists \mathscr{B} \text{ base di } V \text{ formata di autovettori di } f 
$$
$$ \Updownarrow $$
$$
\text{Spettro}(f) \neq \emptyset \text{ e } \sum_{j=1}^m d_j = \dim V = n
$$</p>
<hr>
<h3 id="endomorfismi-diagonalizzabili">Endomorfismi Diagonalizzabili</h3>
<p>Dimostriamo ora che i seguenti fatti sono equivalenti:</p>
<ol>
<li>$\exists \mathscr{B}$ base di $V$ tale che $\mathscr{M}_\mathscr{B}^\mathscr{B}(f)$ sia diagonale</li>
<li>$\exists \mathscr{B}$ base di $V$ formata di autovettori di $f$</li>
<li>$\text{Spettro}(f) = \{ \lambda_1, \dots, \lambda_m \} \neq \emptyset$ e $V = V_{\lambda_1} \oplus \dots \oplus V_{\lambda_m} \quad \lambda_i \neq \lambda_j \iff i \neq j$</li>
</ol>
<ul>
<li><p>$\boxed{\text{i} \implies \text{ii}}$</p>
<p>  <strong>Dim.</strong></p>
<p>  Sappiamo che esiste per ipotesi una base $\mathscr{B}$ di $V$ tale che la matrice del cambiamento di base dalla base in se stessa è diagonale
  $$\mathscr{B} = \{ v_1, \dots, v_n \}$$
  $$
	\mathscr{M}_\mathscr{B}^\mathscr{B}(f)
	=
	\begin{bmatrix}
		\mu_1 & & 0 \\
		& \ddots & \\
		0 & & \mu_n
	\end{bmatrix}
	=
	\begin{bmatrix}
		f(v_1)]_\mathscr{B} & \dots & [f(v_n)]_\mathscr{B}
	\end{bmatrix}
	$$</p>
<p>  Ora basta calcolare l&#39;endomorfismo nei vettori della base e notare che sono tutti autovettori</p>
<p>  $$
	\begin{aligned}
	f(v_1) 
	&= (\text{[ ]}_\mathscr{B}^{-1} \compose \mathscr{M}_\mathscr{B}^\mathscr{B}(f) \compose \text{[ ]}_\mathscr{B})(v_1) \\
	&= (\text{[ ]}_\mathscr{B}^{-1} \compose \mathscr{M}_\mathscr{B}^\mathscr{B}(f))(\begin{bmatrix} 1 \\ 0 \\ \vdots \\ 0 \end{bmatrix}) \\
	&= (\text{[ ]}_\mathscr{B}^{-1})(\begin{bmatrix} \mu_1 \\ 0 \\ \vdots \\ 0 \end{bmatrix}) \\
	&= \mu_1 v_1
	\end{aligned}
	$$</p>
<p>  $$ \implies f(v_1) = \mu_1 v_1 \quad \dots \quad f(v_n) = \mu_n v_n $$</p>
<p>  $\qed$</p>
<blockquote>
<p><strong>TODO:</strong> Forse togliere tutti i passaggi in questa dimostrazione (è banale?)</p>
</blockquote>
</li>
<li><p>$\boxed{\text{ii} \iff \text{iii}}$</p>
<p>  <strong>Dim.</strong></p>
<p>  Per l&#39;ipotesi ii abbiamo che $\exists\mathscr{B} = \{ v_1, \dots, v_n \}$ base di autovettori di $f$. </p>
<p>  $$
	\mathscr{M}_\mathscr{B}^\mathscr{B}(f)
	=
	\begin{bmatrix}
		\lambda_1 I_{d_1} 	& 	   		& 	0			  	\\
						  	& \ddots 	&   				\\
			0			 	& 	   		& \lambda_m I_{d_m} \\
	\end{bmatrix}
	$$</p>
<p>  (con la condizione solita che $\lambda_i \neq \lambda_j \iff i \neq j$ e $d_1 + \dots + d_m = n$) </p>
<p>  Sappiamo che $\text{Spettro}(f) = \text{Spettro}(A)$, proviamo ora a cercare dei $\mu \in \KK$ per cui $\dim \text{Ker}(A - \mu I_n) > 0 \implies \text{rango}(A - \mu I_n) < n$</p>
<p>  $$
	A - \mu I_n 
	=
	\begin{bmatrix}
		(\lambda_1 - \mu) I_{d_1} & & 0 \\
		& \ddots & \\
		0 & & (\lambda_m - \mu) I_{d_m} \\
	\end{bmatrix}
	$$</p>
<p>  Possiamo vedere che $\mu \neq \lambda_j \quad \forall j = 1 \dots m \implies \text{rango}(A - \mu I_n) = n$ e quindi che $\mu \notin \text{Spettro}(f)$</p>
<p>  Inoltre $\mu = \lambda_j \implies \dim \text{Ker}(A - \lambda_j I) = d_j$</p>
<p>  Ed abbiamo così dimostrato la freccia verso destra infatti avremo che </p>
<p>  $$
	\text{Spettro}(f) = \{ \underset{d_1}{\lambda_1}, \underset{\dots}{\dots}, \underset{d_m}{\lambda_m} \} \qquad \sum d_j = n
	$$</p>
<blockquote>
<p><strong>TODO:</strong> Per l&#39;altra freccia basta ripercorrere al contrario la stessa dimostrazione poiché in realtà abbiamo sempre dei $\iff$ (?)</p>
</blockquote>
</li>
</ul>
<p><strong>Lemma./Corollario.</strong></p>
<p>$$ 
f \text{ diagonalizzabile}
\iff
V = V_{\lambda_1} \oplus \dots \oplus V_{\lambda_m}
$$</p>
<p><strong>Lemma.</strong></p>
<ol>
<li>$$ f \sim g $$
 $$ \Downarrow $$
 $$ f \text{ diagonalizzabile} \iff g \text{ diagonalizzabile} $$</li>
<li>$$f \text{ e } g \text{ diagonalizzabili}$$
 $$ \Downarrow $$
 $$
	f \sim g \iff 
	\begin{gathered}
		\text{Spettro}(f) = \text{Spettro}(g) \\
		\forall \lambda \text{ autovalore} \quad d_\lambda(f) = d_\lambda(g)
	\end{gathered}
	$$</li>
</ol>
<p>Abbiamo trovato un&#39;insieme di endomorfismi diagonalizzabili $\mathscr{D}(V) \underset{\normalsize{=}\mathllap{?\,}}{\subset} \text{End}(V)$ e siamo riusciti a classificare almeno questo pezzo</p>
<p>$$
\mathscr{D}(V)/_{ \sim }
$$</p>
<hr>
<h2 id="endomorfismi-non-diagonalizzabili">Endomorfismi non diagonalizzabili</h2>
<p>Un esempio universale che al momento fuoriesce dalla nostra classificazione si può trovare in $V = \KK^2$</p>
<p>$$ A : \KK^2 \to \KK^2 $$
$$ A = \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} $$</p>
<p>Infatti questo endomorfismo ha $\text{Spettro}(A) = \{ 0 \}$ solo che se calcoliamo la dimensione dell&#39;autospazio relativo a $0$ otteniamo</p>
<p>$$
A - \mu \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} = 
\begin{bmatrix} -\mu & 1 \\ 0 & -\mu \end{bmatrix}
$$</p>
<p>E $\mu = 0 \implies d_0 = 2 - \text{rango} \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} = 1$</p>
<p>Possiamo anche fare un esempio peggiore, consideriamo le seguenti due matrici</p>
<p>$$
A =
\begin{bmatrix}
	0 & 1 & 0 & 0 \\
	0 & 0 & 1 & 0 \\
	0 & 0 & 0 & 0 \\
	0 & 0 & 0 & 0 \\
\end{bmatrix}
\qquad
B =
\begin{bmatrix}
	0 & 1 & 0 & 0 \\
	0 & 0 & 0 & 0 \\
	0 & 0 & 0 & 1 \\
	0 & 0 & 0 & 0 \\
\end{bmatrix}
$$</p>
<p>Lo spettro è sempre $\text{Spettro}(A) = \text{Spettro}(B) = \{ 0 \}$ e $d_0(A) = d_0(B) = 2$. Dobbiamo provare a trovare altri invarianti, notiamo intanto che se $0 < n \in \mathbb{N}$</p>
<p>$$
f \sim g \implies f^n \sim g^n
$$</p>
<p><strong>Dim.</strong></p>
<p>$$
g = h \compose f \compose h^{-1}
$$
$$
g^n = h \compose f \compose h^{-1} \compose h \compose f \compose h^{-1} \compose \cdots \compose h \compose f \compose h^{-1} = h \compose f^n \compose h^{-1}
$$</p>
<p>$\qed$</p>
<p>E possiamo ora rianalizzare l&#39;esempio precedente e notare che</p>
<p>$$
A^2 =
\begin{bmatrix}
	0 & 0 & 1 & 0 \\
	0 & 0 & 0 & 0 \\
	0 & 0 & 0 & 0 \\
	0 & 0 & 0 & 0 \\
\end{bmatrix}
{=}\mathllap{/\,} 0
\qquad
B^2 =
\begin{bmatrix}
	0 & 0 & 0 & 0 \\
	0 & 0 & 0 & 0 \\
	0 & 0 & 0 & 0 \\
	0 & 0 & 0 & 0 \\
\end{bmatrix}
= 0
$$
$$ \implies A \not\sim B $$</p>
<blockquote>
<p><strong>TODO:</strong> Classificazione di forza bruta delle matrici 1x1 e 2x2 e guarda caso esce la formula del determinante 2x2</p>
</blockquote>
<h2 id="determinante">Determinante</h2>
<h3 id="studio-in-nodisp-buffer-143-">Studio in $\KK$</h3>
<p>Se proviamo classificare tutti gli endomorfismi di $V = \KK^1$ otteniamo come abbiamo già visto che le uniche due classi di equivalenza sono</p>
<p>$$
\begin{array}{lll}
& & a_{1, 1} = 0 \implies \text{ non invertibile} \\
& \nearrow & \\	
[a_{1,2}] & & \\
& \searrow & \\	
& & a_{1, 1} \neq 0 \implies \text{ invertibile} \\
\end{array}
$$</p>
<p>E quindi possiamo definire una funzione $D_1$ tale che </p>
<p>$$ D_1 : M(1, \KK) \to \KK $$
$$ D_([a_{1,1}]) = a_{1,1} $$
$$ [a_{1,1}] \text{ invertibile} \iff D_1([a_{1,1}]) \neq 0 $$</p>
<p>Inoltre la funzione $D_1$ è <em>lineare</em> e vale $D_1([1])=1$</p>
<h3 id="studio-in-nodisp-buffer-148-">Studio in $\KK^2$</h3>
<p>Se proviamo a fare la stessa cosa con $V = \KK^2$ e applichiamo l&#39;algoritmo di Gauss aggiungendo tutte le varie condizioni ad i passaggi necessari otteniamo</p>
<p>$$
\begin{bmatrix} 
a_{1, 1} & a_{1, 2} \\
a_{2, 1} & a_{2, 2} \\
\end{bmatrix}
\text{ invertibile }
\iff
a_{1, 1} a_{2, 2} - a_{1, 2} a_{2, 1} 
\neq 0
$$</p>
<p>Quindi la nostra funzione $D_2 : M(2, \KK) \to \KK$ sarà</p>
<p>$$
D_2\left(\begin{bmatrix} 
a_{1, 1} & a_{1, 2} \\
a_{2, 1} & a_{2, 2} \\
\end{bmatrix}\right) = a_{1, 1} a_{2, 2} - a_{1, 2} a_{2, 1} 
$$</p>
<p>Ed avremo che $A$ è invertibile $\iff D_2(A) \neq 0$. Possiamo anche intendere la funzione come una funzione sulle colonne</p>
<p>$$
D_2(A) = D_2(A_1, A_2) = 
D_2\left(
\begin{bmatrix} a_{1,1} \\ a_{2,1} \end{bmatrix},
\begin{bmatrix} a_{1,2} \\ a_{2,2} \end{bmatrix}
\right)
$$</p>
<p>La funzione in questo caso però non è lineare ma <em>bilineare</em> infatti</p>
<p>$$
D_2(\lambda A_1, \lambda A_2) = \lambda^2 D_2(A_1, A_2)
$$</p>
<p>Ciò significa che se fisso una colonna e faccio variare l&#39;altra ottengo una applicazione lineare della colonna in $\KK$. Ricapitoliamo ora le proprietà che abbiamo trovato per $D_2$</p>
<ol>
<li>$D_2$ è bilineare (rispetto alle colonne)</li>
<li>$D_2(X, X) = 0$</li>
<li>$D_2(I) = 1$</li>
</ol>
<h4 id="propriet-derivate-da-quelle-di-base">Proprietà derivate da quelle di base</h4>
<ul>
<li><p>$D_2(A_1, A_2) = -D(A_2, A_1)$</p>
<p>  <strong>Dim.</strong></p>
<p>  $$D_2(A_1 + A_2, A_1 + A_2) \overset{\text{(2)}}{=} 0 $$
  $$ \underbrace{D_2(A_1, A_1)}_{=\,0} + D_2(A_1, A_2) + D_2(A_2, A_1) + \underbrace{D_2(A_2, A_2)}_{=\,0} = 0$$
  $\qed$</p>
</li>
<li><p><strong>Prop. (Unicità)</strong> $D_2$ è l&#39;unica funzione che verifica le proprietà i, ii e iii.</p>
<p>  <strong>Dim.</strong></p>
<p>  Ipotizziamo che esista un&#39;altra $F : \KK^2 \times \KK^2 \to \KK$ che verifica le tre proprietà e mostriamo che $\forall A = (A_1, A_2) \implies F(A) = D_2(A)$</p>
<p>  $$
	A =
	\begin{bmatrix}
		a_{1,1} & a_{1,2} \\
		a_{2,1} & a_{2,2}
	\end{bmatrix}
	=
	\begin{bmatrix}
		a_{1,1} E_1 + a_{2,1} E_2 & 
		a_{1,2} E_1 + a_{2,2} E_2
	\end{bmatrix}
	$$</p>
<p>  Dove consideriamo $\KK^2$ nella sua base canonica $\mathscr{C} = \{ E_1, E_2 \}$. Prooviamo ora a calcolare $F(A)$ usando solo le tre proprietà che abbiamo assunto</p>
<p>  $$
	\begin{aligned}
		F(\begin{bmatrix}
			a_{1,1} & a_{1,2} \\
			a_{2,1} & a_{2,2}
		\end{bmatrix})
		&\overset{\text{(1)}}{=}
		a_{1,1} a_{1,2} \underbrace{F(E_1,E_1)}_{=\,0} + a_{1,1} a_{2,2} F(E_1,E_2) \\ & + a_{2,1} a_{1,2} F(E_2,E_1) + a_{2,1} a_{2,2} \underbrace{F(E_2,E_2)}_{=\,0} \\
		&= a_{1,1} a_{2,2} \underbrace{F(I_2)}_{=\,1} + a_{2,1} a_{1,2} \underbrace{F(E_2, E_1)}_{=-F(I)=-1} \\
		&= a_{1,1} a_{2,2} - a_{2,1} a_{1,2} = D_2(A)		
	\end{aligned}
	$$</p>
<p>  $\qed$</p>
</li>
<li><p>Se scegliamo $F_\lambda : \KK^2 \times \KK^2 \to \KK$ in modo che verifichi la prima e la seconda proprietà e tale che $F_\lambda (I_2) = \lambda$ allora otteniamo che $F_\lambda(A) = \lambda D_2(A)$ e possiamo definire</p>
<p>  $$
	\Lambda (\KK^2) = \{ F : \KK^2 \times \KK^2 \to \KK \text{ che verificano i) e ii)} \}
	$$</p>
<p>  E&#39; uno spazio vettoriale con $\dim \Lambda = 1$ e $D_2$ ne è una base. </p>
</li>
</ul>
<h4 id="formula-del-prodotto-di-binet">Formula del prodotto di <em>Binet</em></h4>
<p>$$
D_2(B A) = D_2(B) D_2(A) = D_2(A) D_2(B) = D_2(A B)
$$</p>
<p><strong>Dim.</strong></p>
<p>Consideriamo una $F : \KK^2 \times \KK^2 \to \KK$ in modo che $F(A) \coloneqq D_2(B A)$ abbiamo che</p>
<p>$$
D_2(B A) = F(A) \overset{\text{s.v.}}{=} F(I) D_2(A) = D_2(B) D_2(A)
$$</p>
<p>$\qed$</p>
<blockquote>
<p><strong>TODO.</strong> Rendere la dimostrazione più chiara</p>
</blockquote>
<h4 id="altre-propriet-">Altre proprietà</h4>
<p><strong>Prop.</strong> </p>
<p>$$ A \text{ invertibile } \iff D_2(A) \neq 0 $$</p>
<p><strong>Dim.</strong></p>
<ul>
<li><p>$\boxed{\Leftarrow}$</p>
<p>  Dimostriamo che $A \text{ non invertibile} \implies D_2(A) = 0$. Possiamo assumere che $A_2 = \lambda A_1$</p>
<p>  $$ D_2(A_1, \lambda A_1) = \lambda D_2(A_1, A_1) = 0 $$</p>
<p>  $\qed$</p>
</li>
<li><p>$\boxed{\Rightarrow}$</p>
<p>  Ora dimostriamo invece che $A \text{ invertibile} \implies D_2(A) \neq 0$. Il fatto che $A$ sia invertibile ci dice che esiste $A^{-1}$ tale che $A A^{-1} = I$ e quindi</p>
<p>  $$ D_2(A A^{-1}) = D_2(I) = 1 $$
  $$ \tag{*} D_2(A) D_2(A^{-1}) = 1$$ </p>
<p>  E serve che entrambi i termini di $\text{(*)}$ siano non zero affinché sia vera</p>
<p>  $\qed$</p>
</li>
</ul>
<p><strong>Corollario.</strong> $D_2(A^{-1}) = D_2(A)^{-1}$</p>
<p><strong>Prop.</strong> $A, B \in M(2, \KK) : B \sim A \implies D_2(B) = D_2(A)$</p>
<p><strong>Dim.</strong></p>
<p>$$ \exists P \in GL(2, \KK) : B = P A P^{-1} $$
$$ 
\begin{aligned}
D_2(B) 
&= D_2(P A P^{-1}) = D_2(P) D_2(A) D_2(P^{-1}) \\
& = D_2(P) D_2(P^{-1}) D_2(A) = D_2(P P^{-1}) D_2(A) \\
& = D_2(I) D_2(A) = D_2(A) 
\end{aligned}
$$</p>
<p>$\qed$</p>
<h3 id="studio-in-nodisp-buffer-189-">Studio in $\KK^n$</h3>
<p><strong>Def.</strong></p>
<p>Sia $A \in M(n, \KK) = \underbrace{\KK^n \times \cdots \times \KK^n}_{n}$ con $n \geq 1$ ed $A = [A_1, \dots, A_n]$. Una <strong>funzione determinante</strong> a livello $n$ è $D_n : \underbrace{\KK^n \times \cdots \times \KK^n}_{n} \to \KK$ tale che</p>
<ol>
<li>$D_n$ è $n$-lineare</li>
<li>$D_n(\dots X, X \dots) = 0$</li>
<li>$D_n(I) = 1$</li>
</ol>
<h4 id="propriet-">Proprietà</h4>
<ul>
<li><p>$D_n(\dots X,Y \dots) = -D_n(\dots Y,X \dots)$</p>
<p>  <strong>Dim.</strong></p>
<p>  Basta sviluppare la seguente usando la <em>prima</em> proprietà $D_n( \dots X+Y,X+Y \dots ) \overset{\text{(2)}}{=} 0$ e si ottiene</p>
<p>  $$D_n(\dots X,Y \dots) + D_n(\dots Y,X \dots) = 0$$</p>
<p>  $\qed$</p>
</li>
</ul>
<p>Le due seguenti si mostrano usando la precedente ed <em>agitando le mani</em></p>
<ul>
<li><p>$D_n(\dots X \dots X \dots) = 0$ </p>
</li>
<li><p>$D_n(\dots X \dots Y \dots) = -D_n(\dots Y \dots X \dots)$</p>
</li>
</ul>
<p><strong>Prop.</strong> Se $D_n$ esiste è <strong>unico</strong> e ne possiamo trovare una formula esplicita.</p>
<blockquote>
<p><strong>Gruppo delle permutazioni</strong></p>
<p>$S_n = $ gruppo delle permutazioni su $\{ 1, \dots, n \} = \NN_n$</p>
<p>Valgono le seguenti proprietà</p>
<ul>
<li>$|S_n| = n!$</li>
<li>$S_n \ni \sigma : \{ 1, \dots, n \} \to \{ 1, \dots, n \}$ è bigettiva</li>
<li>$(S_n \; \compose)$ è un gruppo</li>
</ul>
</blockquote>
<p><strong>Dim.</strong></p>
<p>$\{ E_1, \dots, E_n \}$ base canonica di $\KK^n$, con $A \in M(n, \KK)$</p>
<p>$$
A = [A_1, \dots, A_n] \qquad A = [a_{i, j}]_{\tiny\begin{array}{l} i \in \NN_n \\ j \in \NN_n \end{array}}
$$</p>
<p>$$
A =
\left[
\;
\sum_{i \in \NN_n}{a_{i,1} E_i} 
\quad \dots \quad
\sum_{i \in \NN_n}{a_{i,n} E_i} 
\; 
\right]
$$</p>
<!-- 
> **Test in Latex:** Versione ancora più compatta della formula precedente usando notazioni personali bll
> $$
A =
\left[
\sum_{\NN_n}{a_{\square,j} E_\square} 
\right]_{j \in \NN_n}
$$
> $$
A =
(+ / a_{i,j} E_i :_i \NN_n) :_j \NN_n
$$
 -->

<p>Sviluppando completamente $D_n(A)$ usando la $n$-linearità e cancellando i casi in cui le colonne sono uguali otteniamo</p>
<p>$$
\sum_{\sigma \in S_n} a_{\sigma(1),1} \dots a_{\sigma(n),n} D_n([E_{\sigma(1)} \dots E_{\sigma(n)}])
$$</p>
<p>Possiamo ad esempio vedere il caso $n = 2: S_2 = \{ \text{id}, \begin{bmatrix} 1 & 2 \\ 2 & 1 \end{bmatrix} \}$ che infatti risulta essere </p>
<p>$$
a_{1,1} a_{2,2} D_2{E_1, E_2} + a_{2,1} a_{1,2} D_2{E_2, E_1}
$$</p>
<p>In generale $D_{\xi_\sigma} = (-1)^{S(AG)}$ dove l&#39;esponente è il numero di scambi da fare per raggiungere l&#39;identità nell&#39;algoritmo di Gauss. E quindi se $D_n$ esiste abbiamo che $\forall A \in M(n, \KK)$</p>
<p>$$
\sum_{\sigma \in S_n} a_{\sigma(1),1} \dots a_{\sigma(n),n} (-1)^{S(AG)}
$$</p>
<blockquote>
<p><strong>TODO.</strong> Bisognerebbe tipo dimostrare che questa va bene e rispetta le tre proprietà</p>
</blockquote>
<h4 id="definizione-ricorsiva">Definizione ricorsiva</h4>
<p>Possiamo anche procedere per induzione per definire il determinante, per $n=1$ abbiamo banalmente una funzione. Ora supponiamo di aver definito $D_n$ e costruiamo una definizione di $D_{n+1}$</p>
<p>Consideriamo le colonne di una matrice $n+1 \times n+1$ e fissiamo un indice di riga $i$.
$$
A = [ A_1, \dots, A_{n+1} ]
$$</p>
<p>Sia $A_{i,j}$ la sottomatrice $n \times n$ ottenuta cancellando la $i$-esima riga e la $j$-esima colonna.</p>
<p>Definiamo ora per induzione $F_i(A)$ nel seguente modo</p>
<p>$$
F_i(A) \coloneqq \sum_{j=1}^{n+1} a_{i,j} D_n(A_{i,j}) (-1)^{i+j}
$$</p>
<p><strong>Prop.</strong> Per ogni scelta di $i$, $F_i$ verifica le tre proprietà di $D_{n+1}$ e risulta $F_i = D_{n+1}$.</p>
<p><strong>Dim.</strong></p>
<ol>
<li><p>Nel caso $n=2$ con $i=1$ abbiamo
 $$
	\begin{aligned}
		F_1\left(\begin{bmatrix}
				a_{1,1} & a_{1,2} \\
				a_{2,1} & a_{2,2}
			\end{bmatrix}\right) &= \sum_{j=1}^2 a_{1,j} \det (A_{1,j}) (-1)^{i+j} \\
			&= a_{1,1} a_{2,2} (-1)^{1+1} + a_{1,2} a_{2,1} (-1)^{1+2} \\
			&= a_{1,1} a_{2,2} - a_{1,2} a_{2,1} \\
	\end{aligned}
	$$</p>
<p> $\qed$</p>
</li>
<li><p>Ora vediamo che nel caso induttivo $F_i$ verifica la $(n+1)$-linearità.</p>
<blockquote>
<p><strong>TODO.</strong> Scoprire dove sia finita la dimostrazione</p>
</blockquote>
</li>
</ol>
<h3 id="cramer">Cramer</h3>
<p>Vogliamo risolvere il problema $A x = B$, dove $A \in \text{GL}(n, \KK)$. La soluzione è unica ed è</p>
<p>$$
x = A^{-1} B
$$ </p>
<p>Consideriamo le colonne di $A$ ed abbiamo $[ A_1, \dots, A_n ] x = B$, se $x = \[ x_1, \dots, x_n \]^t$ è la soluzione abbiamo che </p>
<p>$$
B = x_1 A_1 + \dots + x_n A_n
$$</p>
<p>$$
\begin{aligned}
	D(A_1 \dots x_1 A_1 + \dots + x_n A_n \dots A_n) \\
	& = D(A_1 \dots x_j A_j \dots A_n) \\
	& = x_j D(A_1 \dots A_n)
\end{aligned}
$$</p>
<p>E quindi</p>
<p>$$
x_j = \frac{D(A_1 \dots A_{j-1} B A_j \dots A_n)}{\underbrace{D(A)}_{\neq 0}}
$$</p>
<h3 id="formula-determinante-per-nodisp-buffer-243-">Formula determinante per $A^{-1}$</h3>
<p>Sia $A \in \text{GL}(n, \KK) \implies A^{-1} = [X_1, \dots, X_n]$ e vogliamo trovare le $X_i$. Sappiamo che $A [ X_1, \dots, X_n ] = [ E_1, \dots, E_n ] = I_n$. Basta quindi risolvere gli $n$ sistemi lineari che abbiamo ricavato.</p>
<p>$$
\begin{cases}
	A X_1 = E_1 \\
	\vdots \\
	A X_n = E_n \\
\end{cases}
$$</p>
<p>Ogni $X_i = [ x_{1, 1}, \dots, x_{n, 1} ]^t \implies$</p>
<p>$$
	x_{i, j} = \frac{\det(A_1 \dots A_{i - 1} E_j A_i \dots A_n)}{\det(A)}
$$</p>
<h3 id="interpretazione-geometrica-del-determinante">Interpretazione Geometrica del determinante</h3>
<h4 id="orientazione">Orientazione</h4>
<h4 id="area-del-parallelogramma">Area del parallelogramma</h4>
<h2 id="altri-invarianti">Altri invarianti</h2>
<p>Per ora sappiamo che lo <em>spettro</em> e le <em>molteplicità geometriche</em> di $f$ sono degli invarianti per simulitudine. Inoltre possiamo vedere che</p>
<p>$$
\lambda \in \text{Spettro}(A) \iff \text{rango}(A - \lambda I) < n \iff \det (A - \lambda I) = 0
$$</p>
<p><strong>Def.</strong>
Si dice <strong>matrice caratteristica</strong> di $A$
$$
A - t I \in M(n, \KK[t])
$$
e <strong>polinomio caratteristico</strong> di $A$
$$
p_A (t) = \det(A - t I) \in \KK[t]
$$</p>
<p><strong>Lemma.</strong> $\lambda \in \text{Spettro}(A) \iff \lambda$ è una radice del polinomio caratteristico di $A$.</p>
<p><strong>Def.</strong> Ora possiamo definire la <strong>molteplicità algebrica</strong> di $\lambda \in \text{Spettro}(A)$ (ovvero $p_A(\lambda) = 0$).
$$
p_A(t) = (\lambda - t)^{m_\lambda} Q(t) \qquad \text{ con } Q(\lambda) \neq 0
$$
Ed in questo caso $m_\lambda$ è detta molteplicità algebrica della radice $\lambda$ di $p_A(t)$.</p>
<p><strong>Prop.</strong> $p_A(t)$ è un invariante per coniugazione.</p>
<div style="height: 300px;"></div>

<script>

// Fix for correctly remembering scrollPos

// setTimeout(() => {
//     window.scrollTo(0, 100000);
// }, 1000);

const scrollPos = localStorage.getItem("mathematics.scroll");

if (scrollPos) {
    setTimeout(() => {
            window.scrollTo(0, parseInt(scrollPos));
    }, 1000);
}

setInterval(() => {
    localStorage.setItem("mathematics.scroll", window.pageYOffset);
}, 1000);

</script>


		<script
			src="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.js"
			integrity="sha384-U8Vrjwb8fuHMt6ewaCy8uqeUXv4oitYACKdB0VziCerzt011iQ/0TqlSlv8MReCm"
		crossorigin="anonymous"></script>
		<script
			src="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/contrib/auto-render.min.js"
			integrity="sha384-aGfk5kvhIq5x1x5YdvCp4upKZYnA8ckafviDpmWEKp4afOZEqOli7gqSnh8I6enH"
		crossorigin="anonymous"></script>
		<script>
			// Needed to fix the z-index of images within paragraph.
			// It looks like that because images are inside sibiling
			// paragraphs one cannot put then in front of the text
			// of the sibiling paragraphs.
			Array.prototype.slice.call(document.querySelectorAll('p'))
				.filter(it => !!it.querySelector('img'))
				.forEach(figure => figure.style.zIndex = 1);
		</script>
		<script>
			renderMathInElement(document.body, {
				delimiters: [
					{left: "$$", right: "$$", display: true},
					{left: "$", right: "$", display: false},
				],
				macros: {
					'\\KK': '\\mathbb{K}',
					'\\NN': '\\mathbb{N}',

					'\\qed': '\\square',
					
					'\\dim': '\\text{dim} \\,',
					'\\Kernel': '\\text{Ker} \\,',
					'\\Image': '\\text{Im} \\,',
					
					'\\taleche': '\\; | \\;',
					'\\compose': '\\, \\circ \\,'
				}
			});
		</script>
	</body>
</html>